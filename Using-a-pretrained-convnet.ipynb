{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Using-a-pretrained-convnet.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_aTzgytWYx4X"},"source":["# Using a pre-trained convnet for image classification\n","\n","We now learn how to use powerful deep learning models, specifically, *pre-trained convnets*, to classify more realistic image datasets (as compared to the overly simplified Fashion MNIST dataset). We will use the popular [dogs vs. cats](https://www.kaggle.com/c/dogs-vs-cats/overview) dataset.\n","\n","This hands-on is based on the code in Chapter 5, Section 3 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?), and simplified by the professor to focus on the key ideas & implementation behind utilizing a pretrained convnet.\n","\n"]},{"cell_type":"code","metadata":{"id":"XLb3fPtcYx4L"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","print(f'tensorflow version is {tf.__version__}')\n","print(f'keras version is {keras.__version__}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wj5Q8Y7oYx4Y"},"source":["## Importing a pre-trained convnet\n","\n","As we discussed in class, the benefit of a pre-trained convnet is that it helps with better feature extraction. We will use a pre-trained convnet called VGG16. Other pretrained convnets available in Keras are :\n","\n","* Xception\n","* InceptionV3\n","* ResNet50\n","* VGG19\n","* MobileNet\n","\n","Let's instantiate the VGG16 model:"]},{"cell_type":"code","metadata":{"id":"2j6deca2Yx4Z"},"source":["from tensorflow.keras.applications import VGG16\n","\n","conv_base = VGG16(include_top=False, input_shape=(150, 150, 3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yLHqzPNnYx4c"},"source":["We passed two arguments to the constructor:\n","\n","* `include_top`, which refers to including or not the densely-connected classifier on top of the network. By default, this \n","densely-connected classifier would correspond to the 1000 classes from ImageNet. Since we intend to use our own densely-connected \n","classifier (with only two classes, cat and dog), we don't need to include it.\n","* `input_shape`, the shape of the image tensors that we will feed to the network. This argument is purely optional: if we don't pass it, \n","then the network will be able to process inputs of any size.\n","\n","Note that this VGG16 model is already pre-trained with the imagenet dataset, thus all trained weight values are already available within the model. Usually we don't touch these weight values. But if needed, the `\"weights\"` parameter allows you to replace or re-train these weights.\n","\n","Here's the detail of the architecture of the VGG16 convolutional base: it's very similar to the simple convnets that you are already \n","familiar with."]},{"cell_type":"code","metadata":{"id":"YqKgyZ3wYx4c"},"source":["conv_base.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E9CMvOO0Yx4g"},"source":["The final feature map has shape `(4, 4, 512)`. That's the feature on top of which we will stick a densely-connected classifier.\n","\n","**Note**: If your input images are *not* of size 150*150, the shape of the final feature map may change accordingly. So to be safe, always check what it will be.\n","\n","At this point, there are two ways we could proceed: \n","\n","* Running the convolutional base over our dataset, recording its output to a Numpy array on disk, then using this data as input to a \n","standalone densely-connected classifier similar to those you have seen in the first chapters of this book. This solution is very fast and \n","cheap to run, because it only requires running the convolutional base once for every input image, and the convolutional base is by far the \n","most expensive part of the pipeline. However, for the exact same reason, this technique would not allow us to leverage data augmentation at \n","all.\n","* Extending the model we have (`conv_base`) by adding `Dense` layers on top, and running the whole thing end-to-end on the input data. This \n","allows us to use data augmentation, because every input image is going through the convolutional base every time it is seen by the model. \n","However, for this same reason, this technique is far more expensive than the first one.\n","\n","We will only try the first technique. "]},{"cell_type":"markdown","metadata":{"id":"HDiV7ChzxSUv"},"source":["## Prepare the dataset\n","We use a smaller dataset of the images of dogs and cats -- 2000 in training, 1000 in validation and 1000 in testing. About 90MB in total size. I share this dataset with you in the form of a single zip file in Canvas. First download and save this file onto your computer.\n","\n","Next, we need to upload this zip file from your computer to Google Colab. Notes:\n","* Why upload this zip file, instead of the 4,000 individual image files? -- The reason is that Google Colab is notorious for being very slow in uploading many small files. This \"zip then unzip\" approach get data ready for us in a few minutes (as compared to 20-30 minutes if we don't zip before uploading).\n","* Why not just mount our Google Drive as we did before? -- Same reason as above, it is very slow to transfer many small files from Google Drive to Google Colab.\n","\n","There are two methods to upload, as below:\n"]},{"cell_type":"markdown","metadata":{"id":"YU8b7GEb1O6_"},"source":["### (Not used today) Method 1 for uploading data into Google Colab\n","\n","The code below uploads this zip file into Google Colab's virtual machine.\n"]},{"cell_type":"code","metadata":{"id":"xX_gYosLsTph"},"source":["from google.colab import files\n","# import io\n","\n","uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yEQzhyYevJca"},"source":["for k,v in uploaded.items():\n","  open(k,'wb').write(v)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LJw2GTg02bdK"},"source":["### Method 2 for uploading data into Google Colab\n","\n","Instead of coding, use the \"Upload to session storage\" button on the left panel. We'll use this method today as it is much faster that Method 1."]},{"cell_type":"markdown","metadata":{"id":"Y_n0VThh3zQw"},"source":["### Unzip the file"]},{"cell_type":"code","metadata":{"id":"J1gQiW9BwQKA"},"source":["# By default, uploaded files are saved under directory /content\n","# The unzip command below creates a folder /content/dogs_vs_cats_small\n","# that contains all the images.\n","!unzip -q /content/dogs_vs_cats_small.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EFMuibyk5ZJs"},"source":["## Feature extraction using the pre-trained convnet\n"]},{"cell_type":"code","metadata":{"id":"JhB7kJlPnAyu"},"source":["import os\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ymrAq1p1Yx4g"},"source":["base_dir = '/content/dogs_vs_cats_small'\n","\n","train_dir = os.path.join(base_dir, 'train')\n","validation_dir = os.path.join(base_dir, 'validation')\n","test_dir = os.path.join(base_dir, 'test')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GYJZi5SXlr7q"},"source":["\n","datagen = ImageDataGenerator(rescale=1./255)\n","batch_size = 100\n","\n","def extract_features(directory, sample_count):\n","    features = np.zeros(shape=(sample_count, 4, 4, 512))\n","    labels = np.zeros(shape=(sample_count))\n","    generator = datagen.flow_from_directory(\n","        directory,\n","        target_size=(150, 150),\n","        batch_size=batch_size,\n","        class_mode='binary')\n","    i = 0\n","    for inputs_batch, labels_batch in generator:\n","        features_batch = conv_base.predict(inputs_batch)\n","        features[i * batch_size : (i + 1) * batch_size] = features_batch\n","        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n","        i += 1\n","        if i * batch_size >= sample_count:\n","            # Note that since generators yield data indefinitely in a loop,\n","            # we must `break` after every image has been seen once.\n","            break\n","    return features, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qCn6h050yybB"},"source":["train_features, train_labels = extract_features(train_dir, 2000)\n","validation_features, validation_labels = extract_features(validation_dir, 1000)\n","test_features, test_labels = extract_features(test_dir, 1000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z65glwq44fm2"},"source":["train_features.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uMchEcMDYx4j"},"source":["The extracted features are currently of shape `(samples, 4, 4, 512)`. We will feed them to a densely-connected classifier, so first we must \n","flatten them to `(samples, 8192)`:"]},{"cell_type":"code","metadata":{"id":"Q-pDkh5zYx4k"},"source":["train_features = np.reshape(train_features, (2000, 4 * 4 * 512))\n","validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))\n","test_features = np.reshape(test_features, (1000, 4 * 4 * 512))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KTl1qiXWYx4m"},"source":["## Classification\n","At this point, we can define our densely-connected classifier (note the use of dropout for regularization), and train it on the data and \n","labels that we just recorded:"]},{"cell_type":"code","metadata":{"id":"BpkrobMsYx4n"},"source":["from tensorflow.keras import models\n","from tensorflow.keras import layers\n","from tensorflow.keras import optimizers\n","\n","model = models.Sequential()\n","model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n","model.add(layers.Dropout(0.5))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n","              loss='binary_crossentropy',\n","              metrics=['acc'])\n","\n","history = model.fit(train_features, train_labels,\n","                    epochs=30,\n","                    batch_size=20,\n","                    validation_data=(validation_features, validation_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gbJtNbveYx4q"},"source":["Training is very fast, since we only have to deal with two `Dense` layers -- an epoch takes less than one second even on CPU.\n","\n","Let's take a look at the loss and accuracy curves during training:"]},{"cell_type":"code","metadata":{"id":"RMATUyAGYx4r"},"source":["import matplotlib.pyplot as plt\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2PIIWdSyYx4u"},"source":["\n","We reach a validation accuracy of about 90%, which is already impressive. \n","\n","However, our plots also indicate that we are overfitting almost from the start -- despite using dropout with a fairly large rate. This is because our dataset is very small in size (for an image classification task). Training over a larger dataset will help alleviating overfitting.\n","\n","Another possible way to improve is to leverage data augmentation, a topic covered in Chapter 5.2 of Chollet's book (not required)."]},{"cell_type":"markdown","metadata":{"id":"YhaYlQwT-QJ4"},"source":["### Performance of the trained classifier\n","\n","Let's see how the trained classifier performs on the hold-out test dataset:"]},{"cell_type":"code","metadata":{"id":"XG6CYvel-FDv"},"source":["test_loss, test_acc = model.evaluate(test_features, test_labels)\n","print('test acc:', test_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QdiYx2Ae3tW1"},"source":["Once a model is trained, we can save and load it whenever needed (rather than to repeatedly train it everytime).\n","\n","Note: If you want to try the save/load code below, you need to first mount your Google Drive to Colab."]},{"cell_type":"code","metadata":{"id":"oQEgL2kg3-82"},"source":["## This is how to save a trained model. Note that you might need to change the path.\n","# model.save('/content/drive/My Drive/AMA/cats_and_dogs_small_model.h5')\n","\n","## This is how to load a trained model. For now I commented it out to avoid accidental mistake.\n","# from tensorflow.keras.models import load_model\n","# model = load_model('/content/drive/My Drive/AMA/cats_and_dogs_small_model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IW3gzCVM26oE"},"source":["## Prediction"]},{"cell_type":"code","metadata":{"id":"VyauUHSI29lN"},"source":["img_path = '/content/dogs_vs_cats_small/test/cats/cat.1700.jpg'\n","\n","# We preprocess the image into a 4D tensor\n","from keras.preprocessing import image\n","import numpy as np\n","\n","img = image.load_img(img_path, target_size=(150, 150))\n","img_tensor = image.img_to_array(img)\n","img_tensor = np.expand_dims(img_tensor, axis=0)\n","# Remember that the model was trained on inputs\n","# that were preprocessed in the following way:\n","img_tensor /= 255.\n","\n","# Its shape is (1, 150, 150, 3)\n","print(img_tensor.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H3jFILbsAYoa"},"source":["features_extracted = conv_base.predict(img_tensor)\n","features_flattened = np.reshape(features_extracted, (1, 4 * 4 * 512))\n","predicted = model.predict(features_flattened)\n","predicted"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9pIGaxxZ444b"},"source":["import matplotlib.pyplot as plt\n","\n","plt.imshow(img_tensor[0])\n","plt.show()"],"execution_count":null,"outputs":[]}]}