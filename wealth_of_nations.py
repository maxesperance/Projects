# -*- coding: utf-8 -*-
"""WEALTH_OF_NATIONS

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bn5bBPNRZ3poVniXd6mMxcT55NdasVcj

# **USING ML TO SCRUB INFO FROM The Wealth of Nations by Adam Smith :**
"""

# Commented out IPython magic to ensure Python compatibility.
#Import necessary packages
import numpy as np 
import pandas as pd 
from scipy.stats import iqr as sc
import statsmodels.api as sm
pd.set_option('max_columns', 100)

#visualizing
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from shapely.geometry import Point, Polygon
import plotly.express as px
import seaborn as sns
import numpy as np
import seaborn as sns
sns.set()
# %matplotlib inline

#metrics and split
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

#model imports
from sklearn import linear_model
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import  ExtraTreesRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LassoCV
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV
from sklearn.metrics import mean_squared_error, r2_score

import tensorflow as tf
import tensorflow_hub as hub
print("TF version: ", tf.__version__)
print("Hub version: ", hub.__version__)

import sys
import nltk
import os
import nltk.corpus

# Importing Porterstemmer from nltk library
import nltk
from nltk.stem import PorterStemmer
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('omw-1.4')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk.probability import FreqDist

#upload file
from google.colab import files
uploaded = files.upload()

#Decode file and save
file_name = "WEALTHofNATIONS.rtf"
article = uploaded[file_name].decode("utf-8")
article = uploaded[file_name].decode("utf-8").split("\r\n")

import resource
import os

#import text file

txt_file = '/content/drive/MyDrive/Colab Notebooks/WEALTHofNATIONS.rtf'

print(f'File Size is {os.stat(txt_file).st_size / (1024 * 1024)} MB')

fileread = open(txt_file)

count = 0

for line in fileread:
    #process file line by line here,take count of lines
    count += 1

fileread.close()

print(f'Number of Lines in the file is {count}')

print('Peak Memory Usage =', resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)
print('User Mode Time =', resource.getrusage(resource.RUSAGE_SELF).ru_utime)
print('System Mode Time =', resource.getrusage(resource.RUSAGE_SELF).ru_stime)

#with open('/content/drive/MyDrive/Colab Notebooks/WEALTHofNATIONS.rtf', 'r') as f: 
    #print(f.read())

with open('WEALTHofNATIONS.rtf', 'r') as file:
    file_df = file.read().strip()
file_df

#file_df.strip('\n\t')
#print(file_df.strip())

import textwrap
#print(textwrap.fill(file_df))

#Lexicon Normalization
#performing stemming and Lemmatization
from nltk.stem.wordnet import WordNetLemmatizer
lem = WordNetLemmatizer()

from nltk.stem.porter import PorterStemmer
stem = PorterStemmer()

print("NORMALIZED:",lem.lemmatize(file_df))

# Number of lines 
lines = file_df.splitlines()
print("Number of lines: ",len(lines))

#length
len(file_df)

#size
print(sys.getsizeof(file_df))

#frequency distribution of article
fdist = FreqDist(file_df)
print(fdist)

# Frequency Distribution Plot
import matplotlib.pyplot as plt
fdist.plot(30,cumulative=False)
plt.show()

#find most common letters
fdist.most_common(25)

#Checking for words from the list of most common letters
#LENGTH OF TIME DEPENDS ON LENGHT OF TEXT
#stm = ["e", "t", "o","a","n"]
#for word in file_df :
   #print(word)

#Word tokenizer breaks text paragraph into words.
tokenized_word=word_tokenize(file_df)
#print(tokenized_word)

#frequency distribution of words
fdist1 = FreqDist(tokenized_word)
print(fdist1)

# Frequency Distribution Plot 
import matplotlib.pyplot as plt
fdist1.plot(30,cumulative=False)
plt.show()

#find most commong key words
fdist1.most_common(25)

# split the text
#splitted_text = file_df.split() 
#print("Splitted text: ",splitted_text)

# words which starts with "I":
words_start_with_I = [word for word in tokenized_word if word.startswith("I")]
print("Words with I: ",words_start_with_I)

# words which starts with "M":
words_start_with_M = [word for word in tokenized_word if word.startswith("M")]
print("Words with M: ",words_start_with_M)

# words which starts with "R":
words_start_with_R = [word for word in tokenized_word if word.startswith("R")]
print("Words with R: ",words_start_with_R)

# words which starts with "E":
words_start_with_E = [word for word in tokenized_word if word.startswith("E")]
print("Words with E: ",words_start_with_E)

# words which starts with "W":
words_start_with_W = [word for word in tokenized_word if word.startswith("W")]
print("Words with W: ",words_start_with_W)

#UNIQUE WORDS
print("unique words: ",set(tokenized_word))

import itertools
import time
import sys
import sys
import os

#separate each features in line
for i in range(len(article)):
  article[i] = article[i].split(",")

print(article)

#Transposing 
new_mat=zip(*article)
for row in new_mat:
    print(row)

#flatten
#b = list(itertools.chain.from_iterable(article))
#print(b)

#tokenizing
#text= file_df
#tokenized_text=sent_tokenize(text)
#SO = print(tokenized_text)

# split the text
#splitted_text = file_df.split() # default split methods splits text according to spaces
#print("Splitted text: ",splitted_text)

# normalization
#words = file_df
words_list = file_df.lower().split(" ")
#print("normalized words: ",words_list)

import nltk as nlp

# stemming
stemming_word_list = file_df
porter_stemmer = nlp.PorterStemmer()
roots = [porter_stemmer.stem(each) for each in stemming_word_list]
#print("Stemming Words: ",roots)

nltk.download('omw-1.4')

# lemmatization
lemma = nlp.WordNetLemmatizer()
lemma_roots = [lemma.lemmatize(each) for each in stemming_word_list]
#print("Lemmatization: ",lemma_roots)

# stemming
porter_stemmer = nlp.PorterStemmer()
roots = [porter_stemmer.stem(each) for each in words_list]
#print("Root Stemming: ",roots)

# !pip install sentencepiece
!pip install bert-for-tf2
import bert