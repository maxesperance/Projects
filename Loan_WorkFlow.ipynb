{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML_2_BasicWorkFlow.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1W35wFc9wPi-ica25OymjN_GHnzex_fPw","authorship_tag":"ABX9TyPsLzAFjTtvdxK0bejJI4Db"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vQc-yqOSALtF"},"source":["# Machine Learning\n","# Part 2: The Basic Work Flow in Python\n","\n","We will together walk through a very basic supervised learning process that involves all the major steps except for the deployment step. \n","\n","Important: This lecture is prepared for students who are *already* experienced with predictive analytics using R. "]},{"cell_type":"markdown","metadata":{"id":"kRFG14lxALtK"},"source":["## 1\\. Data gathering ~~and wrangling~~\r\n","\r\n","The dataset we use is the **Lending Club** dataset. Refer to file \"loan_data_description.pdf\" (that you downloaded from Canvas) for details.\r\n","\r\n","As a very basic code, we won't get into data wrangling yet today. (We don't even know what to wrangle with yet!) \r\n","\r\n","**Important:** Before running the code below, make sure:\r\n","+ you have uploaded the data file \"loan_data.csv\" into folder AMA/02a_ML_in_Python of your online Google Drive,\r\n","+ and you have mounted your Google Drive in this Colab session.\r\n","  + If not yet, click the \"Files\" icon on the left, then click \"mount Drive\"."]},{"cell_type":"markdown","metadata":{"id":"gyaYUjfIALtL"},"source":["### Load the LendingClub dataset"]},{"cell_type":"code","metadata":{"id":"H8f5MpncALtL"},"source":["# Import the necessary Python packages (we'll discuss these packages later). This is analogous to library() in R.\n","import numpy as np\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ku3ADolKALtM"},"source":["# Load the Lending Club dataset. Analogous to read.csv() in R.\n","loan = pd.read_csv('/content/drive/MyDrive/AMA/02a_ML_in_Python/loan_data.csv')\n","\n","# Show on screen the first few records in this dataset. Analogous to head() in R, yet now as a method of object \"loan\".\n","loan.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"91gbrrTUALtK"},"source":["### Data structure for supervised learning in Python\n","\n","Similar to R, in Python we usually expect data in a ***table format*** for predictive analytics, as shown above.\n","+ each row is a **sample**/record, e.g. a customer\n","+ each column is an input variable/attribute/**feature**/predictor, e.g., FICO score of a customer\n","  + with the exception of one column being the output **target**/label /prediction/\"dependent variable\"\n","\n","(**Bold font** above indicates terms most frequently used in the Python ML world.)\n","\n","Two Python packages, **NumPy** and **pandas**, together support our data wrangling needs. We'll delve deeper into them later.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VQ0EtSJ0od1l"},"source":["Different from R, in Python we usually expect the inputs and output to be stored separately as follows.\n","\n"]},{"cell_type":"code","metadata":{"id":"679W8vTwALtM"},"source":["# Separate the dataset into a features matrix X and a target array y\n","X = loan.drop(columns=['not_fully_paid'])\n","y = loan['not_fully_paid']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aHk1vGmoiq1T"},"source":["**Features matrix** (a.k.a. the inputs)\n","\n","The features matrix is often stored in a variable named `X`. The features matrix is assumed to be two-dimensional, with shape `[n_samples, n_features]`, and is most often contained in a NumPy `array` or a Pandas `DataFrame`. \n","\n","**Target array** (a.k.a. the output)\n","\n","In addition to the feature matrix `X`, we also work with a *target array* (or called *label array*) for supervised learning. By convention we call this array `y`. The target array is usually one dimensional, with length `n_samples`, and is generally contained in a NumPy `array` or Pandas `Series`. Values in the target array can be either continuous or discrete. The target array is we want to *predict from the data*, such as whether a customer will default on a loan."]},{"cell_type":"markdown","metadata":{"id":"NZ0yWe9tALtO"},"source":["### Data wrangling\n","\n","Is data wrangling important for real-life data? Absolutely yes.\n","\n","Are we doing data wrangling today? Bravely no :) ."]},{"cell_type":"markdown","metadata":{"id":"wwI5qL4XALtO"},"source":["## 2\\. Exploratory data analysis (EDA)\r\n","\r\n","In the real world, EDA and related data wrangling will likely take most of your time. For today only, let's keep EDA to a bare minimum."]},{"cell_type":"code","metadata":{"id":"wbs6f2rqqW_0"},"source":["# How large is the dataset?\r\n","loan.shape\r\n","# Analogous to dim() in R."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xta-InSpALtO"},"source":["# Any missing data?\n","loan.isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B5afE1JdALtO"},"source":["# summary statistics of each column. Analogous to summary() in R.\r\n","loan.describe().loc[['mean','std']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tnvSlWczALtP"},"source":["Professor Geng thinks, bravely, that this dataset looks ready for modeling. Let's assume Geng is right, and move on."]},{"cell_type":"markdown","metadata":{"id":"0w-o7tjXALtP"},"source":["## 3\\. Modeling\r\n","\r\n","Most traditional machine learning algorithms (as compared to deep learning) in Python are nicely bundled in the awesome **scikit-learn** package (more on this package later). \r\n","+ If anyone asks me why Python over R, scikit-learn is my top reason\r\n","+ In Python coding this package is named `sklearn`"]},{"cell_type":"markdown","metadata":{"id":"ULXr5Nj8ALtP"},"source":["### Partition the data\r\n","\r\n","This is done using the `train_test_split()` function in `sklearn` package:"]},{"cell_type":"code","metadata":{"id":"ohyQ4csDALtP"},"source":["from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eIkY7DyCALtP"},"source":["# reserve 20% dataset as testing\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HalFebjVALtQ"},"source":["### Choose the learning algorithm\n","\n","The scikit_Learn package offers numerous classification and regression learning algorithms, many of which are state-of-the-art choices. You can find them at (https://scikit-learn.org/stable/supervised_learning.html) -- this will be a topic of later weeks.\n","\n","As the target `not_fully_paid` is discrete: 1 for not fully paid, and 0 for fully paid, the Lending Club problem is a **classification** problem.\n","+ Unlike R, there's no \"factor\" data type in Python. \n","\n","Let's try *logistic regression* that we've learned last semester.\n","+ Analogous to glm(..., family=\"binomial\") or train(..., method=\"glm\", family=\"binomial\") in R.\n","\n","(NOT required for today) scikit-learn is famous for providing high-quality documentations. For example, for logistic regression:\n","+ You can find detailed explanation of the underlying statistical concepts at (https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) \n","+ You can find detailed coding definitions and examples at (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n"]},{"cell_type":"code","metadata":{"id":"gnKu4Z_zALtQ"},"source":["from sklearn.linear_model import LogisticRegression"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sZmL8XXgALtR"},"source":["Since logistic regression cannot handle string data, let us drop column 'purpose' (but is this the proper way to handle?):"]},{"cell_type":"code","metadata":{"id":"ls5hoM7vALtR"},"source":["X_train = X_train.drop(columns=['purpose'])\n","X_test = X_test.drop(columns=['purpose'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CDcrm20dALtR"},"source":["### Choose model hyperparameters\n","\n","Most learning algorithms, including `LogisticRegression` implemented in `sklearn`, have many parameters that need to be explicitly set before we can run them. They are often referred to as **hyperparameters**. Intuitively, our choice of these hyperparameters will control how the learning works.\n","\n","Example: The first hyperparameter of `LogisticRegression` is `penalty`. Our choice of its value will affect how complicated the trained algorithm is -- for example, how many features are eventually selected.\n","\n","For today, we'll ignore the topic of how to choose model hyperparameters, and blindly follow Geng's choice below: "]},{"cell_type":"code","metadata":{"id":"T945eoEeALtR"},"source":["model = LogisticRegression(random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"epX_NhLwALtR"},"source":["### Fit your model (a.k.a. train your model)\n","\n","Now let's fit/train our model. That is, plugging our Lending Club dataset into the chosen learning algorithm, and hopefully get a trained algorithm."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"E2rxtVTnALtR"},"source":["model.fit(X_train,y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ax4ml1GmALtS"},"source":["# Run this code cell to see the coefficients of the trained model:\n","logit_reg_coef = pd.DataFrame(model.coef_[0],index=X_train.columns,columns=['Coef'])\n","logit_reg_coef"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5_DmAoDm_Eoa"},"source":["Comment: In glm() in R, specifying the learning algorithm, specifying the hyperparameters, and training are all done in a single line of code. In scikit-learn, however, these are three separate steps. "]},{"cell_type":"markdown","metadata":{"id":"kQZITIELALtS"},"source":["One weakness of the scikit-learn package, as compared to R packages, is that it is more into prediction and less into the completeness of stats reporting. For example, `LogisticRegression` does not report the p-value. If you need it, try another package `statsmodels` as follows:\r\n","```\r\n","import statsmodels.api as sm\r\n","logit_model=sm.Logit(y_train,X_train)\r\n","result=logit_model.fit()\r\n","print(result.summary())\r\n","```"]},{"cell_type":"markdown","metadata":{"id":"GWJEnjl0ALtS"},"source":["## 4\\. Evaluation\n","\n","Now it's time to see how well our trained algorithm performs. First, we apply the trained model to the testing data `X_test` to get predictions. This is done using the `predict()` method."]},{"cell_type":"code","metadata":{"id":"Ik_2A4O_ALtS"},"source":["y_predict = model.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B0QbRowIALtT"},"source":["Second, we compare the predicted values in `y_predict` with the true values we already have in `y_test` using the `accuracy_score()` function:"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"RH6rUuRzALtT"},"source":["from sklearn.metrics import accuracy_score\n","accuracy_score(y_test, y_predict).round(4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GHF9hyhbALtT"},"source":["Does the accuracy of our trained model, as shown above, appear okay to you?\n","\n","Hint: try `1-y_test.mean()`, and think."]},{"cell_type":"code","metadata":{"id":"fJWBqXLSALtT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ov7eSgMCALtT"},"source":["from sklearn.metrics import confusion_matrix\n","print(\"The confusion matrix is:\")\n","cm = confusion_matrix(y_test, y_predict)\n","print(cm)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ylark9-6ALtT"},"source":["**Chances are, we will not stumble upon an ideal trained model in our first try! Or the first many tries!** This will be a repetitive process of observing the results, asking why and forming our ideas on what next we should try, accordingly going back to data wrangling and EDA and model adjustment, and see if our ideas help or not. This will occupy a big chunk of our time in the next 3 weeks."]},{"cell_type":"markdown","metadata":{"id":"fwkz2kfmNBPT"},"source":["**So, what should we do next regarding this Lending Club analytics problem?**"]},{"cell_type":"markdown","metadata":{"id":"UJ45ZMWEALtU"},"source":["## 5\\. Deployment\n","\n","Once we have a champion model that we are happy with, we move it to production. This typically involves converting markdown files to code files (i.e., from .ipynb file to .py file), setting up proper input/output pipes, and task automation (e.g., running it at 8am everyday automatically). (Not required) You can start from this nice [guide](https://medium.com/@thabo_65610/three-ways-to-automate-python-via-jupyter-notebook-d14aaa78de9)."]},{"cell_type":"markdown","metadata":{"id":"r8QtL-iS-HI-"},"source":["## 6\\. Python packages used in this basic flow"]},{"cell_type":"markdown","metadata":{"id":"jdzj8rOVALtK"},"source":["### scikit-learn\n","\n","scikit-learn (https://scikit-learn.org/) is one of the best-known Python libraries for traditional (a.k.a. non-deep) machine learning. Advantages of scikit-learn include:\n","\n","- A [large selection](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) of machine learning algorithms\n","  - efficient implementation (a.k.a. fast) via NumPy and SciPy\n","- A selection of metrics for measuring model performance\n","- Excellent online documentation with awesome examples\n","- A clean, uniform, and streamlined API. Once you learn how to code with one algorithm, switching to other algorithms is usually straightforward\n","\n","**We will study scikit-learn in two weeks** and use it to build, assess and improve our trained models."]},{"cell_type":"markdown","metadata":{"id":"Aa3ZF4NUNiH5"},"source":["### pandas\r\n","\r\n","Effective machine learning depends on quality data input, which usually doesn't come naturally and requires heavy data wrangling. The pandas package in Python offers a powerful set of data structures and tools for us to manipulate data in a table format. In particular, pandas offer two core data structures. First is **DataFrame**, for example:"]},{"cell_type":"code","metadata":{"id":"-DFAq3UJR2Dt"},"source":["loan"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NqbA2F-FSChR"},"source":["Second is **Series**, for example:"]},{"cell_type":"code","metadata":{"id":"L7Wfwpc9SKjX"},"source":["loan['fico']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SD4V5oMsSXyv"},"source":["In plain English, a DataFrame is a table of data, and a Series is one column of it. "]},{"cell_type":"markdown","metadata":{"id":"yui1Fh2WTlq-"},"source":["Furthermore, both DataFrame and Series are **indexed**. The index can be numerical as above, or string based as below."]},{"cell_type":"code","metadata":{"id":"9xkt0C4XxIv_"},"source":["df = pd.read_csv('/content/drive/MyDrive/AMA/02a_ML_in_Python/students.csv', index_col='Name')\r\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"An1Kes8BUBGf"},"source":["The indexed and table-shaped data structure offered by pandas makes data wrangling highly effective and often easier as compared to other languages. Thus pandas is very popular. **We will study pandas next week** and use it to wrangle data for our Lending Club case."]},{"cell_type":"markdown","metadata":{"id":"fNAGKafbOik7"},"source":["### NumPy\r\n","\r\n","pandas (and often scikit-learn) in turn depends on NumPy, a package that focuses on one thing: *array computing* that is fast and easy. **Today we next study the NumPy package**."]}]}
