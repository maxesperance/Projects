{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"convnets classification.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"f8ANoJBB0qWN"},"source":["# Deep Learning with Convolutional Neural Network (CNN, convnet)\n","\n","We continue with the Fashion MNIST dataset. Last week we tried a traditional densely-connected network that resulted in test accuracy of 88.1%. It will be hard, however, to reach 90% accuracy with traditional models.\n","\n","Today we try deep learning: we'll build a *CNN* (*Convolutional Neural Network*, also referred to as *convnet*). Even though our convnet will be very basic, its accuracy will still easily pass 90% for this dataset.\n","\n","This code is based on the example in Chapter 5, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) by Fran√ßois Chollet. The changes are:\n","1. I use the Fashion MNIST dataset rather than the (digit) MNIST dataset, so we can compare with code \"`NN_using_Keras.ipynb`\" that we tried last week.\n","2. Due to updates to TensorFlow, the code in Chollet's book no longer works. I updated the code to make it work."]},{"cell_type":"code","metadata":{"id":"zlLDyehO0qWQ","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1616674389664,"user_tz":300,"elapsed":2153,"user":{"displayName":"Xianjun Geng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjp2zNPbIYU0DaDSSzFs4-oEwNRuLg3LhJILAgh3RY=s64","userId":"03603659341670711497"}},"outputId":"f00a4286-059a-4ea5-f44d-f7579e1929b5"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","keras.__version__"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.4.0'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"8YckT02w0qWU"},"source":["## Building a convnet: the feature extraction layers\n","\n","The code below show you what a basic convnet looks like. It's a stack of `Conv2D` and `MaxPooling2D` layers. We'll see in a minute what they do concretely. \n","\n","Importantly, a convnet takes as input tensors of shape `(image_height, image_width, image_channels)` (not including the batch dimension). \n","In our case, we will configure our convnet to process inputs of size `(28, 28, 1)`, which is the format of MNIST images. We do this via \n","passing the argument `input_shape=(28, 28, 1)` to our first layer."]},{"cell_type":"code","metadata":{"id":"p3qPvijT0qWV","executionInfo":{"status":"ok","timestamp":1616674401214,"user_tz":300,"elapsed":5885,"user":{"displayName":"Xianjun Geng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjp2zNPbIYU0DaDSSzFs4-oEwNRuLg3LhJILAgh3RY=s64","userId":"03603659341670711497"}}},"source":["from tensorflow.keras import layers\n","from tensorflow.keras import models\n","\n","model = models.Sequential()\n","model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iZT0xy380qWa"},"source":["Let's display the architecture of our convnet so far:"]},{"cell_type":"code","metadata":{"id":"sXY3-7Yd0qWb","colab":{"base_uri":"https://localhost:8080/","height":345},"executionInfo":{"status":"ok","timestamp":1585672013462,"user_tz":300,"elapsed":858,"user":{"displayName":"Xianjun Geng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjp2zNPbIYU0DaDSSzFs4-oEwNRuLg3LhJILAgh3RY=s64","userId":"03603659341670711497"}},"outputId":"5c71cba0-9642-4b91-c0f2-c941fccefe8f"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n","=================================================================\n","Total params: 55,744\n","Trainable params: 55,744\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"bIEJCWkw0qWe"},"source":["You can see above that the output of every `Conv2D` and `MaxPooling2D` layer is a 3D tensor of shape `(height, width, channels)`. The width \n","and height dimensions tend to shrink as we go deeper in the network. The number of channels is controlled by the first argument passed to \n","the `Conv2D` layers (e.g. 32 or 64).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zN4owyHnDZdA"},"source":["## Building a convnet: the classification layers\n","\n","The next step would be to feed our last output tensor (of shape `(3, 3, 64)`) into a densely-connected classifier network like those you are \n","already familiar with: a stack of `Dense` layers. These classifiers process vectors, which are 1D, whereas our output so far is a 3D tensor. \n","So first, we will have to flatten our 3D outputs to 1D, and then add a few `Dense` layers on top:"]},{"cell_type":"code","metadata":{"id":"jCSDGM5a0qWf"},"source":["model.add(layers.Flatten())\n","model.add(layers.Dense(64, activation=tf.nn.relu))\n","model.add(layers.Dense(10, activation=tf.nn.softmax))\n","# model.add(layers.Dense(10))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4nohXG4J0qWi"},"source":["We are going to do 10-way classification, so we use a final layer with 10 outputs and a softmax activation. Now here's what our network \n","looks like:"]},{"cell_type":"code","metadata":{"id":"fWHlFce70qWj","colab":{"base_uri":"https://localhost:8080/","height":454},"executionInfo":{"status":"ok","timestamp":1585672935078,"user_tz":300,"elapsed":838,"user":{"displayName":"Xianjun Geng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjp2zNPbIYU0DaDSSzFs4-oEwNRuLg3LhJILAgh3RY=s64","userId":"03603659341670711497"}},"outputId":"de1088c1-1398-42b5-841c-fdb6e683ed70"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n","_________________________________________________________________\n","flatten (Flatten)            (None, 576)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 64)                36928     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 10)                650       \n","=================================================================\n","Total params: 93,322\n","Trainable params: 93,322\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LvM9MKQ-0qWm"},"source":["As you can see, our `(3, 3, 64)` outputs were flattened into vectors of shape `(576,)`, before going through two `Dense` layers.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EGb_EP7YD3gg"},"source":["# Training the model\n","\n","Now, let's train our convnet on the Fashion MNIST dataset. Note that a lot code we used last week can be reused for this part."]},{"cell_type":"code","metadata":{"id":"3TiYu1Sf0qWn","colab":{"base_uri":"https://localhost:8080/","height":183},"executionInfo":{"status":"ok","timestamp":1585673069714,"user_tz":300,"elapsed":2135,"user":{"displayName":"Xianjun Geng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjp2zNPbIYU0DaDSSzFs4-oEwNRuLg3LhJILAgh3RY=s64","userId":"03603659341670711497"}},"outputId":"f3c75865-d72e-4f5f-e10b-e67818beaf28"},"source":["from tensorflow.keras.datasets import fashion_mnist\n","from tensorflow.keras.utils import to_categorical\n","\n","class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n","               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n","\n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n","\n","train_images = train_images.reshape((60000, 28, 28, 1))\n","train_images = train_images / 255.0\n","\n","test_images = test_images.reshape((10000, 28, 28, 1))\n","test_images = test_images / 255.0\n","\n","# train_labels = to_categorical(train_labels)\n","# test_labels = to_categorical(test_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","8192/5148 [===============================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_K65u_ky0qWq"},"source":["model.compile(optimizer='adam', \n","#              loss='categorical_crossentropy',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EVYDwv9rEFVh"},"source":["Just to make sure: check Colab menu \"Runtime\"/\"Change runtime type\", and make sure the choice for \"Hardware accelerator\" is \"GPU\". With GPU, the next line of code takes about 1 minute; without GPU, easily 10 minutes."]},{"cell_type":"code","metadata":{"id":"LI64cACPhWTg","colab":{"base_uri":"https://localhost:8080/","height":763},"executionInfo":{"status":"ok","timestamp":1585673739260,"user_tz":300,"elapsed":205041,"user":{"displayName":"Xianjun Geng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjp2zNPbIYU0DaDSSzFs4-oEwNRuLg3LhJILAgh3RY=s64","userId":"03603659341670711497"}},"outputId":"4db0f799-76f5-454f-c0de-7fe408a0120c"},"source":["model.fit(train_images, train_labels, epochs=20)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.1321 - accuracy: 0.9508\n","Epoch 2/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.1217 - accuracy: 0.9544\n","Epoch 3/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.1127 - accuracy: 0.9578\n","Epoch 4/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.1046 - accuracy: 0.9610\n","Epoch 5/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0975 - accuracy: 0.9630\n","Epoch 6/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0914 - accuracy: 0.9658\n","Epoch 7/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0844 - accuracy: 0.9684\n","Epoch 8/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0796 - accuracy: 0.9698\n","Epoch 9/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0757 - accuracy: 0.9719\n","Epoch 10/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0722 - accuracy: 0.9728\n","Epoch 11/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0656 - accuracy: 0.9752\n","Epoch 12/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0659 - accuracy: 0.9748\n","Epoch 13/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0592 - accuracy: 0.9776\n","Epoch 14/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0579 - accuracy: 0.9776\n","Epoch 15/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0560 - accuracy: 0.9790\n","Epoch 16/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0533 - accuracy: 0.9799\n","Epoch 17/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0510 - accuracy: 0.9813\n","Epoch 18/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0498 - accuracy: 0.9812\n","Epoch 19/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0485 - accuracy: 0.9818\n","Epoch 20/20\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0492 - accuracy: 0.9818\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f25379f4b70>"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"a9Hqzi8M0qWu"},"source":["Let's evaluate the model on the test data:"]},{"cell_type":"code","metadata":{"id":"9rcTSVSq0qWu","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1585673880629,"user_tz":300,"elapsed":1930,"user":{"displayName":"Xianjun Geng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjp2zNPbIYU0DaDSSzFs4-oEwNRuLg3LhJILAgh3RY=s64","userId":"03603659341670711497"}},"outputId":"e03d51c7-d5bf-4e3a-b217-27d54ea3444a"},"source":["test_loss, test_acc = model.evaluate(test_images, test_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 1s 3ms/step - loss: 0.5436 - accuracy: 0.9077\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Cq9sdYny8I5c"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"M9Uy2m1t0qWx"},"source":["While our densely-connected network from last week had a test accuracy of 88.1%, our basic convnet has a test accuracy of 91.0%. Therefore, we decreased the error rate from 11.9% to 9.0% -- a significant improvement of (11.9-9.0)/11.9 = 24%! "]},{"cell_type":"code","metadata":{"id":"dZSeRz3agLXy"},"source":["(11.9-9.0)/11.9"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bCHwJmlkgMKJ"},"source":[""],"execution_count":null,"outputs":[]}]}